{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_multipatch_ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXUTK+DgJ/Tfvzxn3FjXDt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fdsig/A-Lamp/blob/master/pytorch_multipatch_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30Bkf-pO2KgN"
      },
      "source": [
        "import io\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import copy\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "import torch"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dROvAP7Pnk5B"
      },
      "source": [
        "def run_process(process = None, command = None):\n",
        "    logname = process+'.log'\n",
        "    env = os.environ.copy()\n",
        "    with io.open(logname, 'wb') as writer, io.open(logname, 'rb', 1) as reader:\n",
        "        process = subprocess.Popen(command, stdout=writer, shell=True, env=env)\n",
        "        while process.poll() is None:\n",
        "            sys.stdout.write(reader.read())\n",
        "        # Read the remaining\n",
        "        sys.stdout.write(reader.read())\n",
        "\n",
        "repositories = ['https://github.com/fdsig/image_utils',\n",
        "                'https://github.com/fdsig/A-Lamp']\n",
        "\n",
        "for repo in repositories:\n",
        "    run_process(command='git clone '+repo, process=repo.split('/')[-1])\n",
        "\n",
        "other_fids = [fid for fid in os.scandir('image_utils/')]\n",
        "for fid in other_fids:\n",
        "    shutil.move(fid.path,fid.name)\n",
        "os.system(\n",
        "    'rm convit && rm image_utils && rm -rf convit &&\\\n",
        "     rm -rf MPADA && rm -rf image_utils')\n",
        "\n",
        "import image_getter\n",
        "pull = image_getter.Get_Ava()\n",
        "pull.parse_urls()\n",
        "pull.google_getter()\n",
        "pull.ava_txt()\n",
        "pull.download_ava_files(own_drive=True, \n",
        "                        download=True, \n",
        "                        full=True, \n",
        "                        clear_current=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCGehTbB1X-2",
        "outputId": "d1ab6767-9e7d-4849-e15d-548bd653621f"
      },
      "source": [
        "def meta_process():\n",
        "    df = pd.read_csv('ava_meta_with_int_id_230721.csv')\n",
        "    y_gt = df['mos_float'].values\n",
        "    ids = df['ID'].values\n",
        "    print(len(ids))\n",
        "    y_gt_std, y_gt_mean = np.std(y_gt, axis = 0), np.mean(y_gt, axis = 0)\n",
        "    exclude_below = y_gt_mean-y_gt_std*4\n",
        "    exclude_above = y_gt_mean+y_gt_std*4\n",
        "    ids = ids[np.argwhere(y_gt>=exclude_below)].ravel()\n",
        "    y_gt = y_gt[np.argwhere(y_gt>=exclude_below)].ravel()\n",
        "    print(len(y_gt))\n",
        "    ids = ids[np.argwhere(y_gt<=exclude_above)].ravel()\n",
        "    y_gt = y_gt[np.argwhere(y_gt<=exclude_above)].ravel()\n",
        "    print(len(ids),len(y_gt))\n",
        "    ids_low = ids[np.argwhere(y_gt<4.99)].ravel().astype(int)\n",
        "    ids_high = ids[np.argwhere(y_gt>5.01)].ravel().astype(int)\n",
        "    to_include = np.concatenate((ids_low,ids_high), axis=0)\n",
        "    len(to_include)\n",
        "    return df[df['ID'].isin(to_include)]\n",
        "\n",
        "y_df = meta_process()\n",
        "labels = (\n",
        "          fid.name.split('.')[0]\n",
        "          for path in os.scandir('Images/images') \n",
        "          for fid in os.scandir(path.path))\n",
        "y_g = y_df.to_dict('index')\n",
        "y_g_dict = {str(y_g[pair_key]['ID']):y_g[pair_key] for pair_key in y_g}\n",
        "fids = (i for i in os.scandir('Images/images'))\n",
        "fids = {fid.name.split('.')[0]:{'fid':fid.path} for fid in fids}\n",
        "y_g_dict = {key:{**y_g_dict[key],**fids[key]} for key in fids if key in y_g_dict}\n",
        "\n",
        "class Square:\n",
        "    def __call__(self,img): \n",
        "        dims = np.array(img.size)\n",
        "        m_x = dims[[dims.argmax()]][0]\n",
        "        y_axis_pad = int(m_x-dims[1])//2 ;x_axis_pad = int(m_x-dims[0])//2\n",
        "        y_axis_pad+=1;x_axis_pad+=1\n",
        "        pad = (x_axis_pad, y_axis_pad, x_axis_pad, y_axis_pad)\n",
        "        return torchvision.transforms.functional.pad(img,pad,0,'constant')\n",
        "\n",
        "\n",
        "a_transform = A.Compose([\n",
        "         A.augmentations.transforms.LongestMaxSize(max_size=224),\n",
        "        A.augmentations.transforms.PadIfNeeded(224,224,  always_apply=True),\n",
        "        ])\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import ImageFile\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "class ava_data(Dataset):\n",
        "    def __init__(self, im_dict, state = None, transform=None, a_transform=None):\n",
        "        self.im_dict = im_dict\n",
        "        self.transform = transform\n",
        "        self.a_transform = a_transform\n",
        "        self.files  = list(im_dict.keys())\n",
        "        self.state = state\n",
        "\n",
        "    def __len__(self):\n",
        "        self.filelength = len(self.files)\n",
        "        return self.filelength\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #img_path = self.im_dict[self.files[idx]]['fid']\n",
        "        img = self.im_dict[self.files[idx]]['fid']\n",
        "        img = cv2.imread(img)\n",
        "        if len(img.shape) !=3:\n",
        "            img = np.stack([np.copy(img) for i in range(3)], axis=2)\n",
        "             \n",
        "        #img = self.a_transform(image=img)['image']\n",
        "        # converst to pillow image from arry\n",
        "        # this is faster as open cv reads image \n",
        "        # faster than pillow\n",
        "        # pillow also returns file read errors\n",
        "        # for some image in ava dataset\n",
        "        # cv2 does not. \n",
        "        img = Image.fromarray(img.astype('uint8'), 'RGB')\n",
        "        \n",
        "        img_transformed = self.transform(img)\n",
        "        # gets one hot (binary) thresholded groud truth\n",
        "\n",
        "        label = int(self.im_dict[self.files[idx]]['threshold'])\n",
        "\n",
        "        # uncomment to check that lable and data loading correctly (debug)\n",
        "        #print(label, self.im_dict[self.files[idx]])\n",
        "\n",
        "        \n",
        "\n",
        "        return img_transformed, label\n",
        "\n",
        "batch_size = 10\n",
        "lr = 0.0001\n",
        "gamma = 0.1\n",
        "seed = 42\n",
        "sets = ['test', 'training', 'validation']\n",
        "splits ={\n",
        "    set_: {\n",
        "        im_key:y_g_dict[im_key] for im_key in y_g_dict \n",
        "        if y_g_dict[im_key]['set']==set_\n",
        "        } for set_ in sets\n",
        "         }\n",
        "print(f\"train set n = {len(splits['training'])} \\ntest_list n = {len(splits['test'])}\\nvalidation_list n = {len(splits['validation'])}\")\n",
        "import torchvision\n",
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        Square(),\n",
        "     transforms.Resize((224,224)),\n",
        "     transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.25),\n",
        "     transforms.RandomAutocontrast(p=0.25),\n",
        "     transforms.RandomEqualize(p=0.25),\n",
        "\n",
        "\n",
        "        transforms.ToTensor(),\n",
        "      transforms.Normalize(\n",
        "          [0.485, 0.456, 0.406], \n",
        "         [0.229, 0.224, 0.225]\n",
        "                            )\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transforms = transforms.Compose(\n",
        "\n",
        "    [\n",
        "        Square(),\n",
        "         transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "      torchvision.transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    )\n",
        "    ]\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "test_transforms = transforms.Compose(\n",
        "\n",
        "    [   Square(),\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "      torchvision.transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "        )\n",
        "     \n",
        "\n",
        "    ]\n",
        ")\n",
        "data_splits = {set_:\n",
        "    ava_data(\n",
        "        splits[set_], transform=train_transforms,a_transform=a_transform\n",
        "        ) for set_ in splits\n",
        "        }\n",
        "#Let there be 9 samples and 1 sample in class 0 and 1 respectively\n",
        "labels = [splits['training'][idx]['threshold'] for idx in splits['training']]\n",
        "class_counts = np.bincount(labels)\n",
        "num_samples = sum(class_counts)\n",
        "#corresponding labels of samples\n",
        "class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
        "weights = [class_weights[labels[i]] for i in range(int(num_samples))]\n",
        "sampler = torch.utils.data.WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
        "train_loader = DataLoader(\n",
        "    dataset = data_splits['training'], batch_size=batch_size, sampler=sampler,\n",
        "    shuffle=False)\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    dataset = data_splits['validation'], batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(\n",
        "    dataset = data_splits['test'], batch_size=batch_size, shuffle=True)\n",
        "y_g_dict = {key:y_g_dict[key] for key in list(y_g_dict.keys())}\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(seed)\n",
        "device = 'cuda'\n",
        "dataloaders = {'training':train_loader, 'validation':valid_loader}\n",
        "dataset_sizes = {x: len(data_splits[x]) for x in ['training', 'validation']}\n",
        "\n",
        "device = torch.device(device)\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "255502\n",
            "255411\n",
            "255403 255403\n",
            "train set n = 221649 \n",
            "test_list n = 19711\n",
            "validation_list n = 11670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuGwzzId50T9"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, \n",
        "                num_epochs=None, \n",
        "                model_name = None, did = None):\n",
        "    results = { }\n",
        "    \n",
        "    print(f'currently trianing {model_name}')\n",
        "    print(f'{model_name} will be saved at {did+model_name}')\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['training', 'validation']:\n",
        "            if phase == 'training':\n",
        "                model.train()   # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in tqdm(dataloaders[phase], colour=('#FF69B4')):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'training'):\n",
        "                    outputs = model(inputs,inputs,inputs,inputs,inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'training':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'training':\n",
        "                scheduler.step()\n",
        "\n",
        "            ballance = np.array([])\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            class_preds = outputs.argmax(dim=1) \n",
        "            batch_acc = metrics.balanced_accuracy_score(labels.cpu(), \n",
        "                                                         class_preds.cpu())\n",
        "            ballance = np.append(ballance, batch_acc)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            key = 'epoch_'+str(epoch+1)+'_'+phase\n",
        "            results[key]= {\n",
        "                phase+' loss' : epoch_loss, \n",
        "                phase+' acc': float(epoch_acc.cpu()),\n",
        "                phase+ ' ballance_acc':ballance.mean()\n",
        "                }\n",
        "            print(results)\n",
        "            with open(did+model_name+'.json', 'w') as handle:   \n",
        "                    json.dump(results, handle)\n",
        "\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'validation' and ballance.mean() > best_acc:\n",
        "                best_acc = ballance.mean()\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save({'epoch':epoch, \n",
        "                            'model_state_dict':model.state_dict(),\n",
        "                            'optimizer_state_dict':optimizer.state_dict()\n",
        "                            }, did+model_name)\n",
        "                #model save\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "FZ88IJePDdfo",
        "outputId": "d211a6c5-114c-4754-bf65-dbca6921b0b1"
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import albumentations as A\n",
        "class Multi_patch(nn.Module):\n",
        "    def __init__(self, _model, m_odel, mo_del, mod_el, mode_l):\n",
        "        super(Multi_patch, self).__init__()\n",
        "        # to condense with for loop\n",
        "        self._model = _model\n",
        "        self.m_odel = m_odel\n",
        "        self.mo_del = mo_del\n",
        "        self.mod_el = mod_el\n",
        "        self.mode_l = mode_l\n",
        "        # to check this bit\n",
        "        self.classifier = nn.Linear(10,2)\n",
        "        \n",
        "        \n",
        "    def forward(self, x1, x2,x3,x4,x5):\n",
        "        # to condense to dictionary \n",
        "        x1 = self._model(x1)\n",
        "        x2 = self.m_odel(x2)\n",
        "        x3 = self.mo_del(x3)\n",
        "        x4 = self.mod_el(x4)\n",
        "        x5 = self.mode_l(x5)\n",
        "        models = (x1,x2,x3,x4,x5)\n",
        "        x = torch.cat(models, dim=1)\n",
        "        x = self.classifier(F.relu(x))\n",
        "        return x\n",
        "\n",
        "# Create models and load state_dicts  \n",
        "## condense with for loop  \n",
        "_model = models.resnet18(pretrained=True)\n",
        "m_odel = models.resnet18(pretrained=True)\n",
        "mo_del = models.resnet18(pretrained=True)\n",
        "mod_el = models.resnet18(pretrained=True)\n",
        "mode_l = models.resnet18(pretrained=True)\n",
        "\n",
        "models = [_model, m_odel, mo_del, mod_el, mode_l]\n",
        "\n",
        "for mod in models:\n",
        "    n_ftrs = mod.fc.in_features\n",
        "    mod.fc = nn.Linear(n_ftrs,2)\n",
        "\n",
        "model = Multi_patch(_model,m_odel,mo_del, mod_el, mode_l)\n",
        "model = model.to(device)\n",
        "\"\"\"load = iter(train_loader)\n",
        "x1,x2 = next(load)[0],next(load)[0]\n",
        "output = model(x1, x2)\n",
        "print(output)\"\"\"\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "# scheduler\n",
        "scheduler = StepLR(optimizer, step_size=7, gamma=gamma)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# step size \n",
        "\"\"\"ML Flow\"\"\"\n",
        "train_model(model, criterion, optimizer, scheduler, \n",
        "            num_epochs=5, model_name='ensemble',\n",
        "            did ='/content/drive/MyDrive/0.AVA/results/')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "currently trianing ensemble\n",
            "ensemble will be saved at /content/drive/MyDrive/0.AVA/results/ensemble\n",
            "Epoch 0/4\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|\u001b[38;2;255;105;180m          \u001b[0m| 17/22165 [00:04<1:34:48,  3.89it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-8e180f171060>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m train_model(model, criterion, optimizer, scheduler, \n\u001b[1;32m     58\u001b[0m             \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ensemble'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             did ='/content/drive/MyDrive/0.AVA/results/')\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-6783e463219f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs, model_name, did)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolour\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#FF69B4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-dcc12231e7cf>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m#img_path = self.im_dict[self.files[idx]]['fid']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbBpp76WRM2H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}